{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img align=\"center\" src=\"http://sydney.edu.au/images/content/about/logo-mono.jpg\">\n",
    "</center>\n",
    "<h1 align=\"center\" style=\"margin-top:10px\">Statistical Learning and Data Mining</h1>\n",
    "<h2 align=\"center\" style=\"margin-top:20px\">Week 13 Tutorial: Introduction to PyTorch</h2>\n",
    "<br>\n",
    "\n",
    "This tutorial is an introduction to building and training neural networks with [PyTorch](https://pytorch.org/). We'll build a simple feedforward network for fraud detection and train it by stochastic gradient descent.\n",
    "\n",
    "<a href=\"#1.-Credit-Card-Fraud-Data\">Credit card fraud data</a> <br>\n",
    "<a href=\"#2.-Dataset-and-DataLoader\">Dataset and DataLoader</a> <br>\n",
    "<a href=\"#3.-Building-a-neural-network\">Building a neural network</a> <br>\n",
    "<a href=\"#4.-Training\">Training</a> <br>\n",
    "<a href=\"#5.-Logistic-Regression\">Logistic regression</a> <br>\n",
    "<a href=\"#6.-Validation-results\">Validation results</a> <br>\n",
    "\n",
    "This notebook relies on the following libraries and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Credit card fraud data\n",
    "\n",
    "This tutorial will be based on the [credit card fraud dataset](hhttps://www.kaggle.com/mlg-ulb/creditcardfraud) available from [Kaggle Datasets](https://www.kaggle.com/datasets). Our objective is to detect fraudulent credit card transactions using classification methods. \n",
    "\n",
    "Let's assume the following loss matrix: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Actual/Predicted</th>\n",
    "    <th>Legitimate</th>\n",
    "     <th>Fraud</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Legitimate</th>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Fraud</th>\n",
    "    <td>10</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "That is, we assume that it is much worse for the financial institution to miss a fraudulent transaction than to flag a legitimate transaction as potential fraud.\n",
    "\n",
    "We start by loading and inspecting the data. All features except the transaction amount are the result of a principal components analysis (PCA) transformation of undisclosed predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Data/creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relatively large dataset with 284,807 transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are highly imbalanced: only 492 transactions (0.17%) are fraudulent.  This makes the problem much more challenging than the total number of observations would suggest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With so few observations in the fraud class, we should ideally use cross-validation throughout the analysis. However, this would excessively complicate the code, and we simply create a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response='Class'\n",
    "index_train, index_val  = train_test_split(np.array(data.index), stratify=data[response], train_size=0.8, random_state=1)\n",
    "\n",
    "predictors = list(data.columns[1:-1])  # we won't use the  time variable\n",
    "\n",
    "X_train = data.loc[index_train, predictors].to_numpy()\n",
    "y_train = data.loc[index_train, response].to_numpy()\n",
    "\n",
    "X_valid = data.loc[index_val, predictors].to_numpy()\n",
    "y_valid = data.loc[index_val, response].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step that we need to prepare the data is to standardise the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset and DataLoader\n",
    "\n",
    "When working with PyTorch, we need to tell it how to process the data and construct minibatches for stochastic gradient descent. \n",
    "\n",
    "The first step is to create a PyTorch dataset object. A `Dataset` class must implement three methods: `__init__`, `__len__`, and `__getitem__`. The first takes data as an input, processes it as required, and instantiates the `DataSet` object. The second returns the number of observations. The third takes an index as an input and returns the observation that corresponds to that index.\n",
    "\n",
    "The `__init__` implementation below converts the original NumPy arrays into [PyTorch tensors](https://pytorch.org/docs/stable/tensors.html) and converts them into the required memory format.\n",
    "\n",
    "A tensor in this context is just another name for an array. The PyTorch documentation writes:\n",
    "\n",
    "> A `torch.Tensor` is a multi-dimensional matrix containing elements of a single data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FraudDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, features, response):\n",
    "        self.features =  torch.from_numpy(features).float()\n",
    "        self.response = torch.from_numpy(response).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.response)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx, :], self.response[idx]\n",
    "    \n",
    "train_data = FraudDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) takes a `DataSet` as an input and combines it a sampling strategy to allow PyTorch to iterate over mini-batches.\n",
    "\n",
    " Setting the `shuffle` option to `True` makes the DataLoader reshuffle the data at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = 1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell grabs a randomly sampled mini-batch for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4037, -0.7890,  0.3855,  ...,  0.1138,  0.1171,  0.3953],\n",
       "        [-0.2206,  0.5561, -1.6911,  ...,  0.9195,  0.8567, -0.3148],\n",
       "        [ 0.6056,  0.3252, -0.1057,  ...,  0.0495,  0.1221, -0.3508],\n",
       "        ...,\n",
       "        [ 0.5775,  0.0777,  0.5506,  ...,  0.0917,  0.1075, -0.2349],\n",
       "        [-0.3241,  0.4454,  0.4775,  ..., -1.0179, -0.0201, -0.3428],\n",
       "        [-0.2604,  0.4896,  1.4939,  ..., -0.1925, -0.4826, -0.3463]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_loader))\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 29])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building a neural network\n",
    "\n",
    "Our neural network model will be a feedforward network with three hidden layers. Each layer will have 128 hidden units and the activation function will be the rectified linear unit (ReLU).\n",
    "\n",
    "We need to specify the model as a PyTorch neural network module that is a subclass of [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The model class needs to have an `__init__` method that initialises the neural network layers and `forward` method that implements the operations to be performed on the inputs.\n",
    "\n",
    "The following code takes advantage of the `nn.Sequential` class, which allows us to quickly stack pre-defined layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "   \n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.feedforward = nn.Sequential(            \n",
    "            nn.Linear(29, 128),            \n",
    "            nn.ReLU(),                       \n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )                        \n",
    "\n",
    "\n",
    "    def forward(self, features):        \n",
    "        return self.feedforward(features).flatten() # returns a flat array as desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the model, move it to the GPU (if available), and print the model structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (feedforward): Sequential(\n",
      "    (0): Linear(in_features=29, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "dfn = NeuralNetwork().to(device)\n",
    "\n",
    "print(dfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a GPU, note that it needs to have enough memory to hold the model and a minibatch. This is not a problem here, but in practice you'll often need worry about what your GPU's memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training\n",
    "\n",
    "To train the neural network, it's useful to code a function that loops over the entire training set to co complete one epoch of the optimisation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimiser):\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    n_batches = len(dataloader)\n",
    "    n_obs = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (features, response) in enumerate(dataloader):\n",
    "        \n",
    "        # Move data to GPU, if available\n",
    "        features = features.to(device)\n",
    "        response  = response.to(device)\n",
    "        \n",
    "        # Compute the predictions (forward pass)\n",
    "        prediction = model(features)\n",
    "        \n",
    "        # Evaluate cost function\n",
    "        loss = loss_fn(prediction, response)\n",
    "\n",
    "        # Compute gradient (backward pass)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "\n",
    "        # Print progress\n",
    "        if batch % int(np.floor(0.2*n_batches)) == 0:\n",
    "            loss, current = loss.item(), batch * len(response)\n",
    "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{n_obs:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also compute and print the validation results at the end of every epoch. In this case, we process the validation training set in one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, log_loss, average_precision_score\n",
    "\n",
    "def validation(model):\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Predicted probabilities \n",
    "    # the .cpu().detach().numpy() part transfers the result the cpu and converts it to a numpy array\n",
    "    with torch.no_grad():\n",
    "        y_prob = model(torch.from_numpy(X_valid).float().to(device)).cpu().detach().numpy()\n",
    "    \n",
    "    \n",
    "    # Classification using the decision threshold\n",
    "    tau = 1/11\n",
    "    y_pred = (y_prob > tau).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    nll = log_loss(y_valid, y_prob)\n",
    "    sensitivity = recall_score(y_valid, y_pred)\n",
    "    auprc = average_precision_score(y_valid, y_prob)\n",
    "      \n",
    "\n",
    "    print('')\n",
    "    print('Validation metrics \\n')\n",
    "    print(f\"Loss: {np.round(nll, 4)}\")\n",
    "    print(f\"Sensitivity: {np.round(sensitivity, 3)}\")\n",
    "    print(f\"Average precision: {np.round( auprc, 3)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now finally train the model. We use the Adam optimiser, which is an extension of SGD that often works well as a default optimisation method. \n",
    "\n",
    "The learning rate is based on trial and error, though in practice we can use hyperparameter optimisation tools to select it. \n",
    "\n",
    "To select the number of epochs, we can use early stopping. In this approach, we keep track of the validation metrics at the end of each epoch and stop the learning process when the validation performance stops improving. We don't explicitly code this method below, but the validation metrics tend to stop improving after about five or six epochs for this problem.\n",
    "\n",
    "Note that you will not get the same numbers because of the randomness in the learning algorithm. When using the CPU for training, it's possible to achieve reproducibility by following the [recommendations](https://pytorch.org/docs/stable/notes/randomness.html) in the PyTorch documentation. With GPU training, reproducibility is difficult and often not possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Loss: 0.660927  [    0/227845]\n",
      "Loss: 0.008776  [45056/227845]\n",
      "Loss: 0.014260  [90112/227845]\n",
      "Loss: 0.007543  [135168/227845]\n",
      "Loss: 0.003137  [180224/227845]\n",
      "Loss: 0.004282  [225280/227845]\n",
      "\n",
      "Validation metrics \n",
      "\n",
      "Loss: 0.0036\n",
      "Sensitivity: 0.816\n",
      "Average precision: 0.801 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Loss: 0.016526  [    0/227845]\n",
      "Loss: 0.000512  [45056/227845]\n",
      "Loss: 0.003951  [90112/227845]\n",
      "Loss: 0.001746  [135168/227845]\n",
      "Loss: 0.002246  [180224/227845]\n",
      "Loss: 0.001426  [225280/227845]\n",
      "\n",
      "Validation metrics \n",
      "\n",
      "Loss: 0.0025\n",
      "Sensitivity: 0.888\n",
      "Average precision: 0.845 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Loss: 0.001321  [    0/227845]\n",
      "Loss: 0.010567  [45056/227845]\n",
      "Loss: 0.001628  [90112/227845]\n",
      "Loss: 0.000954  [135168/227845]\n",
      "Loss: 0.010703  [180224/227845]\n",
      "Loss: 0.000702  [225280/227845]\n",
      "\n",
      "Validation metrics \n",
      "\n",
      "Loss: 0.0022\n",
      "Sensitivity: 0.888\n",
      "Average precision: 0.865 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Loss: 0.009099  [    0/227845]\n",
      "Loss: 0.000536  [45056/227845]\n",
      "Loss: 0.000986  [90112/227845]\n",
      "Loss: 0.002042  [135168/227845]\n",
      "Loss: 0.000600  [180224/227845]\n",
      "Loss: 0.000488  [225280/227845]\n",
      "\n",
      "Validation metrics \n",
      "\n",
      "Loss: 0.0024\n",
      "Sensitivity: 0.867\n",
      "Average precision: 0.878 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Loss: 0.000163  [    0/227845]\n",
      "Loss: 0.001048  [45056/227845]\n",
      "Loss: 0.010956  [90112/227845]\n",
      "Loss: 0.011264  [135168/227845]\n",
      "Loss: 0.000743  [180224/227845]\n",
      "Loss: 0.000462  [225280/227845]\n",
      "\n",
      "Validation metrics \n",
      "\n",
      "Loss: 0.0022\n",
      "Sensitivity: 0.888\n",
      "Average precision: 0.876 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = nn.BCELoss() # binary cross-entropy loss\n",
    "optimiser = torch.optim.Adam(dfn.parameters(), lr = learning_rate) # Adam tends to work well as a default\n",
    " \n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, dfn, loss_fn, optimiser)\n",
    "    validation(dfn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# No regularisation\n",
    "logit = LogisticRegression(penalty='none', solver='lbfgs')\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "# L2 regularisation\n",
    "logit_l2= LogisticRegressionCV(Cs = 50, penalty='l2', solver='lbfgs', scoring='neg_log_loss', n_jobs=-1)\n",
    "logit_l2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Validation results\n",
    "\n",
    "The next cell compares the validation performance of the neural network against the logistic regression benchmark. In my results, the neural network significantly outperforms the logistic regression in terms of the estimated risk, average precision, and cross-entropy. \n",
    "\n",
    "Some important comments:\n",
    "\n",
    "(i) As noted above, you will not get the same numbers because of the randomness in the optimisation process. \n",
    "\n",
    "(ii) A useful trick for training neural networks is to re-run the learning algorithm if necessary. You can then select and save a model that performs well on the validation set. The disadvantage of this approach is that it can overfit the validation set. If possible, it's better to average multiple neural networks trained on different training-validation splits, discarding those with poor validation performance. \n",
    "\n",
    "(iii) The comparison with the logistic regression is not entirely rigorous since we looked at the validation performance to select a reasonable number of epochs for training the neural network.\n",
    "\n",
    "(iv) It's not clear why the neural networks seems to perform better. As earlier in the unit, it's important to use EDA and interpretability tools to understand what's happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix,  f1_score\n",
    "\n",
    "columns=['Estimated risk', 'Error rate', 'Sensitivity', 'Specificity', \n",
    "         'Precision', 'Average Precision', 'F1 Score', 'Cross-entropy']\n",
    "rows=['Logistic', 'Logistic $\\ell_2$', 'Neural network']\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "methods=[logit, logit_l2, dfn]\n",
    "\n",
    "lfp = 1\n",
    "lfn = 10\n",
    "tau = lfp/(lfp+lfn)\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    \n",
    "    if i==2:\n",
    "        with torch.no_grad():\n",
    "            y_prob = dfn(torch.from_numpy(X_valid).float().to(device)).cpu().detach().numpy()\n",
    "    else:\n",
    "        y_prob = method.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    y_pred = (y_prob>tau).astype(int)\n",
    "       \n",
    "    tn, fp, fn, tp = confusion_matrix(y_valid, y_pred).ravel()\n",
    "    \n",
    "    results.iloc[i,0]=  (fp*lfp+fn*lfn)/len(y_valid)\n",
    "    results.iloc[i,1]=  1 - accuracy_score(y_valid, y_pred)\n",
    "    results.iloc[i,2]=  tp/(tp+fn)\n",
    "    results.iloc[i,3]=  tn/(tn+fp)\n",
    "    results.iloc[i,4]=  precision_score(y_valid, y_pred)\n",
    "    results.iloc[i,5]=  average_precision_score(y_valid, y_prob)\n",
    "    results.iloc[i,6]=  f1_score(y_valid, y_pred)\n",
    "    results.iloc[i,7]=  10*log_loss(y_valid, y_prob)\n",
    "\n",
    "results.iloc[:,0] /= results.iloc[0,0]\n",
    "results.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
